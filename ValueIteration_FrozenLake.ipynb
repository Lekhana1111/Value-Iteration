{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Frozen Lake Domain Description\n",
        "\n",
        "Frozen Lake is a simple grid-world environment where an agent navigates a frozen lake to reach a goal while avoiding falling into holes. The environment is represented as a grid, with each cell being one of the following:\n",
        "\n",
        "* **S**: Starting position of the agent\n",
        "* **F**: Frozen surface, safe to walk on\n",
        "* **H**: Hole, falling into one ends the episode with a reward of 0\n",
        "* **G**: Goal, reaching it ends the episode with a reward of 1\n",
        "\n",
        "The agent can take four actions:\n",
        "\n",
        "* **0: Left**\n",
        "* **1: Down**\n",
        "* **2: Right**\n",
        "* **3: Up**\n",
        "\n",
        "However, due to the slippery nature of the ice, the agent might not always move in the intended direction. There's a chance it moves perpendicular to the intended direction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hzTUHNC0Oien"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nKf_jjk9OgN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b65adc-9a65-463b-be3d-680abdc4e3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transition model for the Frozen Lake world describes how the agent's actions affect its movement and the resulting state transitions. Here's a breakdown of the key components:\n",
        "\n",
        "**Actions:**\n",
        "\n",
        "* The agent can choose from four actions:\n",
        "    * 0: Left\n",
        "    * 1: Down\n",
        "    * 2: Right\n",
        "    * 3: Up\n",
        "\n",
        "**State Transitions:**\n",
        "\n",
        "* **Intended Movement:** Ideally, the agent moves one cell in the chosen direction.\n",
        "* **Slippery Ice:** Due to the slippery nature of the ice, there's a probability that the agent will move in a perpendicular direction instead of the intended one. The exact probabilities depend on the specific Frozen Lake environment configuration, but typically:\n",
        "    * **Successful Move:** The agent moves in the intended direction with a high probability.\n",
        "    * **Perpendicular Move:** The agent moves 90 degrees to the left or right of the intended direction with a lower probability.\n",
        "* **Boundaries:** If the intended movement would take the agent outside the grid boundaries, it remains in its current position.\n",
        "* **Holes:** If the agent lands on a hole (\"H\"), the episode ends, and it receives a reward of 0.\n",
        "* **Goal:** If the agent reaches the goal (\"G\"), the episode ends, and it receives a reward of 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R_q5-OvYOldL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()\n",
        "print (\"State 14 Going Right: (s, a, r, Done)\", env.P[14][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7nU_-9uaOQR",
        "outputId": "cd0147a2-0812-4565-b136-8b51df1a05ce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "State 14 Going Right: (s, a, r, Done) [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create FrozenLake environment\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "# Value Iteration Algorithm\n",
        "def value_iteration(env, gamma=0.9, num_iterations=1000):\n",
        "    \"\"\"\n",
        "    Implements the Value Iteration algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        gamma: Discount factor.\n",
        "        num_iterations: Number of iterations to run.\n",
        "\n",
        "    Returns:\n",
        "        The optimal value function and policy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize value function and policy\n",
        "    V = np.zeros(env.observation_space.n)  # Value function for all states\n",
        "    policy = np.zeros(env.observation_space.n)  # Policy to store the best action for each state\n",
        "\n",
        "    # Iterate for the number of iterations specified\n",
        "    for i in range(num_iterations):\n",
        "        prev_V = np.copy(V)\n",
        "\n",
        "        # Iterate over all states\n",
        "        for state in range(env.observation_space.n):\n",
        "            Q_values = []  # To store Q-values for all actions\n",
        "\n",
        "            # Calculate Q-values for each action in the current state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_V[next_state])\n",
        "                Q_values.append(q_value)\n",
        "\n",
        "            # Update the value function with the max Q-value\n",
        "            V[state] = max(Q_values)\n",
        "\n",
        "            # Update the policy to choose the action with the highest Q-value\n",
        "            policy[state] = np.argmax(Q_values)\n",
        "\n",
        "        # Check for convergence (optional step)\n",
        "        if np.max(np.abs(V - prev_V)) < 1e-4:\n",
        "            print(f\"Value iteration converged after {i+1} iterations.\")\n",
        "            break\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "# Apply Value Iteration\n",
        "optimal_V, optimal_policy = value_iteration(env)\n",
        "\n",
        "# Evaluate the resulting policy\n",
        "def evaluate_policy(env, policy, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluates the policy by running it for a number of episodes and calculating\n",
        "    the average reward.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        policy: The policy to evaluate.\n",
        "        num_episodes: Number of episodes to run.\n",
        "\n",
        "    Returns:\n",
        "        The average reward obtained by following the policy.\n",
        "    \"\"\"\n",
        "\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "# Evaluate the optimal policy\n",
        "average_reward = evaluate_policy(env, optimal_policy)\n",
        "print(\"Average Reward:\", average_reward)\n"
      ],
      "metadata": {
        "id": "U92a-f0HO1j7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29b2144-8d71-414d-a435-7d2de6dd0527"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value iteration converged after 44 iterations.\n",
            "Average Reward: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Value Iteration Algorithm\n",
        "\n",
        "Value Iteration is a dynamic programming algorithm used to compute the optimal value function and policy for Markov Decision Processes (MDPs).\n",
        "In each iteration, we update the value function by considering the expected rewards and transitions, and the policy is updated based on the action\n",
        "that leads to the maximum Q-value. In this implementation, we loop through all states and actions to find the optimal values and policies.\n"
      ],
      "metadata": {
        "id": "4fXIIATGu1zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Results of Value Iteration\n",
        "\n",
        "After running Value Iteration, we evaluate the optimal policy over 100 episodes. The algorithm converges to an optimal solution based on the\n",
        "discount factor (gamma = 0.9), which balances immediate rewards with future rewards. The 'Average Reward' printed shows the effectiveness\n",
        "of the policy in reaching the goal while avoiding the holes."
      ],
      "metadata": {
        "id": "EFEqX7hPvBgZ"
      }
    }
  ]
}